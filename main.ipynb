{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toxigen_dataset import ToxiGenDataset\n",
    "from hatebert_model import HateBERTClassifier\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as  nn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxigen Dataset\n",
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text', 'target_group', 'labels'],\n",
      "        num_rows: 940\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text', 'target_group', 'labels'],\n",
      "        num_rows: 8960\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Initialize and load the ToxiGen dataset\n",
    "HUGGING_FACE_TOKEN = \"A AJOUTER VOTRE TOKEN HUGGINGFACE\"\n",
    "toxigen_dataset = ToxiGenDataset(HUGGING_FACE_TOKEN)\n",
    "toxigen = toxigen_dataset.load_dataset()\n",
    "print(toxigen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Overview ===\n",
      "Training set size: 8960\n",
      "Test set size: 940\n",
      "Total dataset size: 9900\n",
      "\n",
      "=== Label Distribution ===\n",
      "\n",
      "Training set:\n",
      "labels\n",
      "0    0.624107\n",
      "1    0.375893\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set:\n",
      "labels\n",
      "0    0.568085\n",
      "1    0.431915\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== Target Group Distribution ===\n",
      "\n",
      "Training set:\n",
      "target_group\n",
      "women              717\n",
      "lgbtq              714\n",
      "mental_dis         714\n",
      "black              713\n",
      "chinese            706\n",
      "asian              702\n",
      "native_american    702\n",
      "middle_east        697\n",
      "muslim             688\n",
      "physical_dis       685\n",
      "mexican            684\n",
      "jewish             684\n",
      "latino             554\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set:\n",
      "target_group\n",
      "physical_dis       95\n",
      "black              92\n",
      "jewish             87\n",
      "muslim             83\n",
      "chinese            77\n",
      "mexican            73\n",
      "middle_east        68\n",
      "mental_dis         68\n",
      "lgbtq              66\n",
      "women              65\n",
      "latino             61\n",
      "native_american    54\n",
      "asian              51\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Text Length Statistics ===\n",
      "\n",
      "Training set:\n",
      "count    8960.000000\n",
      "mean       94.775223\n",
      "std        38.130894\n",
      "min         1.000000\n",
      "25%        64.000000\n",
      "50%        98.000000\n",
      "75%       126.000000\n",
      "max       208.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Test set:\n",
      "count    940.000000\n",
      "mean      97.731915\n",
      "std       50.389837\n",
      "min       13.000000\n",
      "25%       67.000000\n",
      "50%       93.000000\n",
      "75%      123.000000\n",
      "max      762.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas DataFrames for easier analysis\n",
    "train_df = pd.DataFrame(toxigen['train'])\n",
    "test_df = pd.DataFrame(toxigen['test'])\n",
    "\n",
    "print(\"=== Dataset Overview ===\")\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"Total dataset size: {len(train_df) + len(test_df)}\")\n",
    "\n",
    "\n",
    "print(\"\\n=== Label Distribution ===\")\n",
    "print(\"\\nTraining set:\")\n",
    "print(train_df['labels'].value_counts(normalize=True))\n",
    "print(\"\\nTest set:\")\n",
    "print(test_df['labels'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n=== Target Group Distribution ===\")\n",
    "print(\"\\nTraining set:\")\n",
    "print(train_df['target_group'].value_counts())\n",
    "print(\"\\nTest set:\")\n",
    "print(test_df['target_group'].value_counts())\n",
    "\n",
    "# Text length statistics\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "test_df['text_length'] = test_df['text'].str.len()\n",
    "\n",
    "print(\"\\n=== Text Length Statistics ===\")\n",
    "print(\"\\nTraining set:\")\n",
    "print(train_df['text_length'].describe())\n",
    "print(\"\\nTest set:\")\n",
    "print(test_df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hatebert_model:Using device: cpu\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at GroNLP/hateBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:hatebert_model:Model and tokenizer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize HateBERT classifier\n",
    "model_hatebert = HateBERTClassifier(\n",
    "    model_name=\"GroNLP/hateBERT\",\n",
    "    num_labels=2,\n",
    "    device=None,  # Will automatically detect cuda/cpu\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function is now handled by HateBERT's prepare_inputs method\n",
    "# We can directly use the tokenizer from our hatebert instance\n",
    "tokenizer = model_hatebert.tokenizer\n",
    "\n",
    "def tokenize_function(tokenizer, example, text_field):\n",
    "    \"\"\"\n",
    "    Tokenize texts using the HateBERT tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: HateBERT tokenizer\n",
    "        example: Dataset example\n",
    "        text_field: Name of the text field to tokenize\n",
    "        \n",
    "    Returns:\n",
    "        Tokenized features\n",
    "    \"\"\"\n",
    "    # Tokenize the texts\n",
    "    tokenized = tokenizer(\n",
    "        example[text_field],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Convert to lists for dataset storage\n",
    "    return {\n",
    "        'input_ids': tokenized['input_ids'].squeeze().tolist(),\n",
    "        'attention_mask': tokenized['attention_mask'].squeeze().tolist(),\n",
    "        'token_type_ids': tokenized['token_type_ids'].squeeze().tolist()\n",
    "    }\n",
    "\n",
    "'''\n",
    " apply the function to all the elements in the dataset (individually or in batches)\n",
    " https://huggingface.co/docs/datasets/v1.11.0/package_reference/main_classes.html?highlight=dataset%20map#datasets.Dataset.map\n",
    " batch mode is very powerful. It allows you to speed up processing\n",
    " more info here: https://huggingface.co/docs/datasets/en/about_map_batch\n",
    "'''\n",
    "cache_files = {\n",
    "    \"test\": \".cache/datasets/toxigen/toxigen_test_tokenized.arrow\",\n",
    "    \"train\": \".cache/datasets/toxigen/toxigen_train_tokenized.arrow\"\n",
    "} #path to the local cache files, where the current computation from the following function will be stored. \n",
    "# Caching saves RAM when working with large datasets and saves time instead of doing transformations on the fly.\n",
    "tokenized_toxigen = toxigen.map(lambda x: tokenize_function(tokenizer, x, \"text\"), batched=True, cache_file_names=cache_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text', 'target_group', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 940\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text', 'target_group', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 8960\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_toxigen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch_dict = {k: [dic[k] for dic in batch] for k in batch[0]}\n",
    "    for k in ['input_ids', 'attention_mask', 'token_type_ids', 'labels']:\n",
    "        if k in batch_dict:\n",
    "            # Convert lists to tensors before stacking\n",
    "            batch_dict[k] = torch.stack([torch.tensor(x) for x in batch_dict[k]])\n",
    "    # Optionally, keep 'text' and 'target_group' as lists of strings if you need them\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a smaller subset of the dataset as previously shown to speed up the fine-tuning\n",
    "small_train_dataset = tokenized_toxigen[\"train\"].shuffle(seed=42).select(range(300))\n",
    "small_eval_dataset = tokenized_toxigen[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataLoader for your training and test datasets so you can iterate over batches of data:\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "test_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mbonh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "INFO:hatebert_model:Epoch 1/3\n",
      "Training:   0%|          | 0/38 [00:00<?, ?it/s]c:\\Users\\mbonh\\Documents\\EPFL\\cours\\MFE_Master_4\\Deep Learning\\Project\\deep_learning2025\\hatebert_model.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs['labels'] = torch.tensor(labels).to(self.device)\n",
      "Training: 100%|██████████| 38/38 [01:23<00:00,  2.21s/it, loss=0.803]\n",
      "INFO:hatebert_model:Epoch 1 - Train Loss: 0.6727, Train Accuracy: 0.5600\n",
      "INFO:hatebert_model:Epoch 2/3\n",
      "Training:   0%|          | 0/38 [00:00<?, ?it/s]c:\\Users\\mbonh\\Documents\\EPFL\\cours\\MFE_Master_4\\Deep Learning\\Project\\deep_learning2025\\hatebert_model.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs['labels'] = torch.tensor(labels).to(self.device)\n",
      "Training: 100%|██████████| 38/38 [01:28<00:00,  2.33s/it, loss=0.526]\n",
      "INFO:hatebert_model:Epoch 2 - Train Loss: 0.5713, Train Accuracy: 0.6667\n",
      "INFO:hatebert_model:Epoch 3/3\n",
      "Training:   0%|          | 0/38 [00:00<?, ?it/s]c:\\Users\\mbonh\\Documents\\EPFL\\cours\\MFE_Master_4\\Deep Learning\\Project\\deep_learning2025\\hatebert_model.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs['labels'] = torch.tensor(labels).to(self.device)\n",
      "Training: 100%|██████████| 38/38 [01:41<00:00,  2.67s/it, loss=0.314]\n",
      "INFO:hatebert_model:Epoch 3 - Train Loss: 0.4879, Train Accuracy: 0.7667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.6726961026066228, 0.571270069793651, 0.4879345933073445],\n",
       " 'train_accuracy': [0.56, 0.6666666666666666, 0.7666666666666667]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hatebert.train(train_dataloader=train_dataloader,\n",
    "                     num_epochs=3,\n",
    "                     learning_rate=2e-5,\n",
    "                     save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/13 [00:00<?, ?it/s]c:\\Users\\mbonh\\Documents\\EPFL\\cours\\MFE_Master_4\\Deep Learning\\Project\\deep_learning2025\\hatebert_model.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs['labels'] = torch.tensor(labels).to(self.device)\n",
      "Testing: 100%|██████████| 13/13 [00:10<00:00,  1.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([[0.6371546 , 0.3628454 ],\n",
       "        [0.4520047 , 0.5479952 ],\n",
       "        [0.6962602 , 0.3037398 ],\n",
       "        [0.53668576, 0.46331427],\n",
       "        [0.6436471 , 0.35635293],\n",
       "        [0.7979959 , 0.20200408],\n",
       "        [0.79391813, 0.20608185],\n",
       "        [0.6720464 , 0.32795355],\n",
       "        [0.7851104 , 0.21488956],\n",
       "        [0.8026144 , 0.19738556],\n",
       "        [0.78238255, 0.21761744],\n",
       "        [0.7546486 , 0.24535131],\n",
       "        [0.72492355, 0.27507642],\n",
       "        [0.6535448 , 0.34645528],\n",
       "        [0.70093304, 0.299067  ],\n",
       "        [0.650606  , 0.34939393],\n",
       "        [0.7866784 , 0.21332163],\n",
       "        [0.5156771 , 0.48432294],\n",
       "        [0.6903167 , 0.30968335],\n",
       "        [0.7536524 , 0.24634755],\n",
       "        [0.59313166, 0.40686837],\n",
       "        [0.64900273, 0.3509973 ],\n",
       "        [0.7749268 , 0.22507322],\n",
       "        [0.60061955, 0.3993805 ],\n",
       "        [0.74347943, 0.2565205 ],\n",
       "        [0.6002117 , 0.3997883 ],\n",
       "        [0.5100501 , 0.48994985],\n",
       "        [0.77230597, 0.22769408],\n",
       "        [0.6919356 , 0.30806437],\n",
       "        [0.79586166, 0.20413832],\n",
       "        [0.6932526 , 0.3067473 ],\n",
       "        [0.4932317 , 0.50676835],\n",
       "        [0.6464284 , 0.35357162],\n",
       "        [0.68364173, 0.31635827],\n",
       "        [0.5831464 , 0.4168536 ],\n",
       "        [0.8440698 , 0.15593022],\n",
       "        [0.7537988 , 0.24620125],\n",
       "        [0.6459714 , 0.35402864],\n",
       "        [0.64880687, 0.3511932 ],\n",
       "        [0.8394367 , 0.16056328],\n",
       "        [0.77054435, 0.22945562],\n",
       "        [0.72010136, 0.27989867],\n",
       "        [0.6830098 , 0.31699023],\n",
       "        [0.6034477 , 0.39655238],\n",
       "        [0.80952865, 0.1904713 ],\n",
       "        [0.54221255, 0.4577875 ],\n",
       "        [0.38579288, 0.61420715],\n",
       "        [0.7056276 , 0.29437232],\n",
       "        [0.77061343, 0.22938663],\n",
       "        [0.6853381 , 0.31466195],\n",
       "        [0.5224943 , 0.47750562],\n",
       "        [0.58344954, 0.4165505 ],\n",
       "        [0.5607672 , 0.43923277],\n",
       "        [0.76330155, 0.23669843],\n",
       "        [0.49984455, 0.5001555 ],\n",
       "        [0.71388507, 0.2861149 ],\n",
       "        [0.51926655, 0.48073345],\n",
       "        [0.6673087 , 0.33269137],\n",
       "        [0.705637  , 0.29436305],\n",
       "        [0.68995863, 0.31004137],\n",
       "        [0.7336321 , 0.26636794],\n",
       "        [0.6152161 , 0.3847839 ],\n",
       "        [0.69627357, 0.30372646],\n",
       "        [0.71945363, 0.2805463 ],\n",
       "        [0.74935746, 0.2506425 ],\n",
       "        [0.71585137, 0.28414872],\n",
       "        [0.56972   , 0.4302801 ],\n",
       "        [0.56736004, 0.43264   ],\n",
       "        [0.71649283, 0.28350714],\n",
       "        [0.7906535 , 0.20934643],\n",
       "        [0.59511703, 0.40488303],\n",
       "        [0.5744413 , 0.42555866],\n",
       "        [0.7706739 , 0.22932607],\n",
       "        [0.7036221 , 0.29637787],\n",
       "        [0.7354042 , 0.26459578],\n",
       "        [0.58744085, 0.41255915],\n",
       "        [0.69111556, 0.3088844 ],\n",
       "        [0.7144601 , 0.2855399 ],\n",
       "        [0.46484858, 0.5351514 ],\n",
       "        [0.6024054 , 0.3975946 ],\n",
       "        [0.5937564 , 0.40624356],\n",
       "        [0.7329227 , 0.26707727],\n",
       "        [0.6170345 , 0.3829655 ],\n",
       "        [0.6805953 , 0.3194047 ],\n",
       "        [0.56541073, 0.43458927],\n",
       "        [0.7414889 , 0.25851113],\n",
       "        [0.7793019 , 0.22069813],\n",
       "        [0.74625695, 0.25374305],\n",
       "        [0.6369051 , 0.36309496],\n",
       "        [0.739002  , 0.260998  ],\n",
       "        [0.47676423, 0.5232357 ],\n",
       "        [0.6918295 , 0.3081705 ],\n",
       "        [0.57526565, 0.42473432],\n",
       "        [0.56570154, 0.4342985 ],\n",
       "        [0.71626234, 0.28373763],\n",
       "        [0.6001805 , 0.39981943],\n",
       "        [0.65472513, 0.34527484],\n",
       "        [0.533517  , 0.466483  ],\n",
       "        [0.5928156 , 0.4071844 ],\n",
       "        [0.6785375 , 0.32146245]], dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hatebert.predict(test_dataloader, return_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 13/13 [00:09<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall metrics:\n",
      "Confusion Matrix:\n",
      "[[59  1]\n",
      " [35  5]]\n",
      "F1 Score: 0.2174\n",
      "\n",
      "Metrics by target group:\n",
      "\n",
      "Target Group: muslim\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [1 0]]\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Target Group: black\n",
      "Confusion Matrix:\n",
      "[[3 0]\n",
      " [6 0]]\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Target Group: jewish\n",
      "Confusion Matrix:\n",
      "[[2 0]\n",
      " [3 1]]\n",
      "F1 Score: 0.4000\n",
      "\n",
      "Target Group: chinese\n",
      "Confusion Matrix:\n",
      "[[6 0]\n",
      " [2 0]]\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Target Group: women\n",
      "Confusion Matrix:\n",
      "[[2 0]\n",
      " [5 0]]\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Target Group: mexican\n",
      "Confusion Matrix:\n",
      "[[7 0]\n",
      " [0 1]]\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Target Group: asian\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [1 0]]\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Target Group: middle_east\n",
      "Confusion Matrix:\n",
      "[[0 1]\n",
      " [3 1]]\n",
      "F1 Score: 0.3333\n",
      "\n",
      "Target Group: native_american\n",
      "Confusion Matrix:\n",
      "[[4 0]\n",
      " [2 0]]\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Target Group: lgbtq\n",
      "Confusion Matrix:\n",
      "[[7 0]\n",
      " [2 1]]\n",
      "F1 Score: 0.5000\n",
      "\n",
      "Target Group: physical_dis\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 5  0]]\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Target Group: mental_dis\n",
      "Confusion Matrix:\n",
      "[[3 0]\n",
      " [3 0]]\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Target Group: latino\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [2 1]]\n",
      "F1 Score: 0.5000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_by_target_group(test_dataloader, predictions, target_groups=None):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix and F1 score for each target group.\n",
    "    \n",
    "    Args:\n",
    "        test_dataloader: DataLoader containing test data\n",
    "        predictions: Model predictions array\n",
    "        target_groups: Optional list of target groups to evaluate specifically\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "    \n",
    "    # Extract all labels and target groups from test dataloader\n",
    "    all_labels = []\n",
    "    all_target_groups = []\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        all_labels.extend(batch['labels'].numpy())\n",
    "        all_target_groups.extend(batch['target_group'])\n",
    "    \n",
    "    # Get unique target groups if not specified\n",
    "    if target_groups is None:\n",
    "        target_groups = list(set(all_target_groups))\n",
    "    \n",
    "    print(\"Overall metrics:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, predictions))\n",
    "    print(f\"F1 Score: {f1_score(all_labels, predictions):.4f}\\n\")\n",
    "    \n",
    "    print(\"Metrics by target group:\")\n",
    "    for group in target_groups:\n",
    "        # Get indices for this target group\n",
    "        group_indices = [i for i, g in enumerate(all_target_groups) if g == group]\n",
    "        \n",
    "        if len(group_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get predictions and labels for this group\n",
    "        group_preds = predictions[group_indices]\n",
    "        group_labels = [all_labels[i] for i in group_indices]\n",
    "        \n",
    "        print(f\"\\nTarget Group: {group}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(group_labels, group_preds))\n",
    "        print(f\"F1 Score: {f1_score(group_labels, group_preds):.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "preds, _ = model_hatebert.predict(test_dataloader, return_probs=True)\n",
    "evaluate_model_by_target_group(test_dataloader, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
